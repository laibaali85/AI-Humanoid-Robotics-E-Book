# Data Model: Module 4: Vision-Language-Action (VLA)

This section outlines the key conceptual entities discussed within Module 4, focusing on their definitions and roles within the Vision-Language-Action (VLA) paradigm for humanoid robotics. This is a conceptual data model for the content, not a software data model.

## Key Entities

### VLA (Vision-Language-Action)
- **Description**: A robotics paradigm where robots interpret multimodal inputs (visual data and natural language commands) to perform physical actions in the real world.
- **Attributes**:
    - `components`: Vision (perception), Language (understanding), Action (execution).
    - `purpose`: Enables more intuitive and flexible human-robot interaction.
- **Relationships**:
    - Integrates: Whisper, LLM, ROS 2 Action Sequences

### Whisper
- **Description**: OpenAI's general-purpose speech recognition model. It takes audio input and transcribes it into text, serving as a key component for converting spoken commands into machine-readable language in VLA systems.
- **Attributes**:
    - `functionality`: Speech-to-Text transcription.
    - `input`: Audio.
    - `output`: Text.
- **Relationships**:
    - Feeds into: LLM (for planning)
    - Part of: VLA Language component

### LLM (Large Language Model)
- **Description**: An advanced AI model trained on vast amounts of text data, capable of understanding, generating, and processing human language. In VLA, LLMs act as high-level planners, translating natural language commands into robot-executable action sequences.
- **Attributes**:
    - `functionality`: Natural language understanding, planning, reasoning.
    - `input`: Text commands (often from Whisper).
    - `output`: ROS 2 Action Sequences.
- **Relationships**:
    - Receives input from: Whisper
    - Generates: ROS 2 Action Sequences
    - Part of: VLA Language/Action component

### ROS 2 Action Sequences
- **Description**: A series of predefined, executable robotic actions or tasks within the ROS 2 framework. These are typically lower-level commands that a robot's controller can directly execute (e.g., "move_arm_to_position", "grasp_object").
- **Attributes**:
    - `granularity`: Robot-executable, often atomic commands.
    - `format`: ROS 2 action messages.
- **Relationships**:
    - Generated by: LLM (as a planner)
    - Executed by: Robot controllers via ROS 2
    - Part of: VLA Action component
