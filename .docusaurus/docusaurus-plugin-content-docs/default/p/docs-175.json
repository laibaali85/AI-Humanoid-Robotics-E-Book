{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Module 1: ROS2 Nervous System","items":[{"type":"link","href":"/docs/module1/chapter1-introduction","label":"Chapter 1: Introduction to ROS 2 (Understanding the Nervous System)","docId":"module1/chapter1-introduction","unlisted":false},{"type":"link","href":"/docs/module1/chapter2-nodes-topics-services","label":"Chapter 2: ROS 2 Nodes, Topics, and Services","docId":"module1/chapter2-nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/module1/chapter3-rclpy","label":"Chapter 3: Connecting with rclpy (Python Client Library)","docId":"module1/chapter3-rclpy","unlisted":false},{"type":"link","href":"/docs/module1/chapter4-urdf","label":"Chapter 4: Describing Robots with URDF","docId":"module1/chapter4-urdf","unlisted":false},{"type":"link","href":"/docs/module1/chapter5-final-project","label":"Chapter 5: Final Project","docId":"module1/chapter5-final-project","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/docs/module2/overview","label":"Module 2: The Digital Twin (Overview)","docId":"module2/overview","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: AI Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/docs/module3/overview","label":"Module 3: The AI-Robot Brain (Overview)","docId":"module3/overview","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision Language Action (VLA)","items":[{"type":"link","href":"/docs/module4/overview","label":"Module 4: Vision-Language-Action (Overview)","docId":"module4/overview","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module1/chapter1-introduction":{"id":"module1/chapter1-introduction","title":"Chapter 1: Introduction to ROS 2 (Understanding the Nervous System)","description":"This chapter introduces ROS 2 (Robot Operating System 2) as the fundamental communication and coordination framework for robots. We'll explore why it's often called the \"nervous system\" of a robot, enabling different components to send and receive information, much like biological systems.","sidebar":"tutorialSidebar"},"module1/chapter2-nodes-topics-services":{"id":"module1/chapter2-nodes-topics-services","title":"Chapter 2: ROS 2 Nodes, Topics, and Services","description":"This chapter dives into the core communication mechanisms within ROS 2. We will learn about nodes, which are the fundamental computational units, and how they communicate using topics for streaming data and services for request-response interactions. Understanding these primitives is key to building complex robot behaviors.","sidebar":"tutorialSidebar"},"module1/chapter3-rclpy":{"id":"module1/chapter3-rclpy","title":"Chapter 3: Connecting with rclpy (Python Client Library)","description":"The rclpy library is ROS 2's Python client. It allows you to write ROS 2 nodes using Python, making it easy for AI engineers to integrate their models and algorithms with a robot's communication system. This chapter will conceptually demonstrate how rclpy is used to create nodes, publish data, and subscribe to information from other parts of the robot.","sidebar":"tutorialSidebar"},"module1/chapter4-urdf":{"id":"module1/chapter4-urdf","title":"Chapter 4: Describing Robots with URDF","description":"The Unified Robot Description Format (URDF) is an XML file format that allows you to describe all elements of a robot. This includes its physical properties (like mass and inertia), visual characteristics (how it looks), and most importantly, its kinematic and dynamic structure (how its parts move relative to each other). This chapter will introduce the basic concepts of URDF, focusing on links (the rigid parts of a robot) and joints (how those parts connect and move).","sidebar":"tutorialSidebar"},"module1/chapter5-final-project":{"id":"module1/chapter5-final-project","title":"Chapter 5: Final Project","description":"This chapter will contain a hands-on project that integrates all the concepts covered in this module. You will learn how to create a ROS 2 package, define custom messages, and use services and actions to build a complete robotics application.","sidebar":"tutorialSidebar"},"module2/overview":{"id":"module2/overview","title":"Module 2: The Digital Twin (Overview)","description":"This chapter introduces the concept of a Digital Twin, which is a virtual model designed to accurately reflect a physical object. We will explore its significance in robotics, especially in simulating complex systems without the need for physical hardware. The module will cover the use of simulation platforms like Gazebo and Unity, demonstrating how they are used to simulate robot physics, collisions, and various environments. Furthermore, we will delve into how robotic sensors such as LiDAR, Depth Cameras, and IMUs are simulated to enable virtual robots to perceive their surroundings.","sidebar":"tutorialSidebar"},"module3/overview":{"id":"module3/overview","title":"Module 3: The AI-Robot Brain (Overview)","description":"This chapter delves into the \"AI-Robot Brain,\" focusing on NVIDIA's Isaac platform. We will explore Isaac Sim for photorealistic simulation and synthetic data generation, which is vital for training AI models. The module also covers Isaac ROS for implementing perception tasks like Visual SLAM (VSLAM) and Nav2 for sophisticated locomotion planning. These tools enable humanoid robots to perceive their environment, understand their location, and navigate autonomously.","sidebar":"tutorialSidebar"},"module4/overview":{"id":"module4/overview","title":"Module 4: Vision-Language-Action (Overview)","description":"This chapter explores the cutting-edge field of Vision-Language-Action (VLA) systems, which enable robots to understand and execute commands based on natural language and visual perception. We will discuss technologies like Whisper for voice-to-action capabilities and how Large Language Models (LLMs) can serve as powerful planners, translating human instructions into sequences of ROS 2 actions. The module culminates in an examination of high-level autonomous humanoid workflows driven by VLA systems.","sidebar":"tutorialSidebar"}}}}